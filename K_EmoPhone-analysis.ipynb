{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Required Packages\n",
    "* Imbalanced data handling: *imbalanced-learn*\n",
    "* Classification model: *xgboost*\n",
    "* Model explanation: *shap*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade pip imblearn xgboost shap Geohash ray optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytz\n",
    "import pickle5 as pickle\n",
    "import cloudpickle\n",
    "import ray\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import mitosheet\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "from functools import wraps\n",
    "from contextlib import contextmanager\n",
    "\n",
    "\n",
    "DEFAULT_TZ = pytz.FixedOffset(540)  # GMT+09:00; Asia/Seoul\n",
    "\n",
    "\n",
    "def load(path: str):\n",
    "    with open(path, mode='rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "    \n",
    "def dump(obj, path: str):\n",
    "    with open(path, mode='wb') as f:\n",
    "        cloudpickle.dump(obj, f)\n",
    "        \n",
    "    \n",
    "def log(msg: any):\n",
    "    print('[{}] {}'.format(datetime.now().strftime('%y-%m-%d %H:%M:%S'), msg))\n",
    "\n",
    "    \n",
    "def show(d: pd.DataFrame):\n",
    "    return mitosheet.sheet(d)\n",
    "\n",
    "\n",
    "def stat(x):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        m = np.mean(x)\n",
    "        s = np.std(x, ddof=1)\n",
    "        l, u = st.t.interval(0.95, len(x) - 1, loc=np.mean(x), scale=st.sem(x))\n",
    "        med = np.median(x)\n",
    "        q_1, q_3 = np.quantile(x, [0.25, 0.75])\n",
    "        iqr = (q_3 - q_1) * 1.5\n",
    "    \n",
    "        return m, s, l, u, med, q_1 - iqr, q_3 + iqr\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def on_ray(*args, **kwargs):\n",
    "    try:\n",
    "        if ray.is_initialized():\n",
    "            ray.shutdown()\n",
    "        ray.init(*args, **kwargs)\n",
    "        yield None\n",
    "    finally:\n",
    "        ray.shutdown()\n",
    "        \n",
    "        \n",
    "def catch_warning(t: str):\n",
    "    def wrapper(f):\n",
    "        @wraps(f)\n",
    "        def decorator(*args, **kwargs):\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(t)\n",
    "                return f(*args, **kwargs)\n",
    "        return decorator\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw records as csv format\n",
    "\n",
    "### Label data\n",
    "* EsmResponse-1966\n",
    "\n",
    "### Demographic information\n",
    "* UserInfo\n",
    "\n",
    "### Smartphone data\n",
    "* AppUsageEvent\n",
    "* AppUsageStat\n",
    "* BatteryEvent\n",
    "* CallEvent\n",
    "* Connectivity\n",
    "* DataTraffic\n",
    "* DeviceEvent\n",
    "* InstalledApp\n",
    "* Location\n",
    "* MediaEvent\n",
    "* MessageEvent\n",
    "* ActivityEvent\n",
    "* ActivityTransition\n",
    "* WiFi\n",
    "\n",
    "\n",
    "### MicrosoftBand2 data\n",
    "* Acceleration\n",
    "* AmbientLight\n",
    "* Calorie\n",
    "* Distance\n",
    "* EDA\n",
    "* HR\n",
    "* StepCount\n",
    "* RRI\n",
    "* SkinTemperature\n",
    "* UltraViolet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The affect state labels (e.g., valence, arousal, etc.) and the corresponding collection times are retrieved from the ESM response data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "l = pd.read_csv('./data/EsmResponse-1966.csv')\n",
    "LABEL = pd.merge(\n",
    "    left=l, \n",
    "    right=l.groupby(\n",
    "        'Pcode', as_index=False\n",
    "    ).mean().rename(lambda x: f'_{x}', axis=1),\n",
    "    left_on='Pcode', \n",
    "    right_on='_Pcode'\n",
    ").assign(\n",
    "    pcode=lambda x: x['Pcode'],\n",
    "    timestamp=lambda x: pd.to_datetime(x['ResponseTime'], unit='ms', utc=True).dt.tz_convert(DEFAULT_TZ),\n",
    "    val_dyn=lambda x: np.where(x['Valence'] > x['_Valence'], 1, 0),\n",
    "    aro_dyn=lambda x: np.where(x['Arousal'] > x['_Arousal'], 1, 0),\n",
    "    att_dyn=lambda x: np.where(x['Attention'] > x['_Attention'], 1, 0),\n",
    "    sts_dyn=lambda x: np.where(x['Stress'] > x['_Stress'], 1, 0),\n",
    "    dst_dyn=lambda x: np.where(x['Disturbance'] > x['_Disturbance'], 1, 0),\n",
    "    val_fix=lambda x: np.where(x['Valence'] > 0, 1, 0),\n",
    "    aro_fix=lambda x: np.where(x['Arousal'] > 0, 1, 0),\n",
    "    att_fix=lambda x: np.where(x['Attention'] > 0, 1, 0),\n",
    "    sts_fix=lambda x: np.where(x['Stress'] > 0, 1, 0),\n",
    "    dst_fix=lambda x: np.where(x['Disturbance'] > 0, 1, 0),\n",
    ").set_index(\n",
    "    ['pcode', 'timestamp']\n",
    ")[[\n",
    "    'val_dyn', 'aro_dyn', 'att_dyn', 'sts_dyn', 'dst_dyn',\n",
    "    'val_fix', 'aro_fix', 'att_fix', 'sts_fix', 'dst_fix'\n",
    "]]\n",
    "\n",
    "\n",
    "dump(LABEL, './proc/label.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "esm = pd.read_csv(\"./data/EsmResponse-1966.csv\")\n",
    "\n",
    "box = sns.boxplot(data=esm[[\"Valence\", \"Arousal\", \"Stress\", \"Attention\", \"Disturbance\", \"Change\"]],\n",
    "                  showmeans=True,\n",
    "                  meanprops={\"marker\":\"o\",\n",
    "                             \"markerfacecolor\":\"white\", \n",
    "                             \"markeredgecolor\":\"black\",\n",
    "                             \"markersize\":\"7\"}\n",
    "                 )\n",
    "# box.title.set_text(f\"Label Distribution (1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "esm = pd.read_csv(\"./data/EsmResponse-1966.csv\")\n",
    "\n",
    "box = sns.boxplot(data=esm[[\"Duration\"]],\n",
    "                  showmeans=True,\n",
    "                  meanprops={\"marker\":\"o\",\n",
    "                             \"markerfacecolor\":\"white\", \n",
    "                             \"markeredgecolor\":\"black\",\n",
    "                             \"markersize\":\"7\"},\n",
    "                  width=.13,\n",
    "                  color='grey'\n",
    "                 )\n",
    "# box.title.set_text(f\"Label Distribution (2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Participants' information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARTICIPANT_INFO = pd.read_csv(\"./data/UserInfo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARTICIPANT_INFO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensor data retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, Optional, Union, Iterable\n",
    "import Geohash as geo\n",
    "import json\n",
    "from datetime import timedelta\n",
    "    \n",
    "\n",
    "def _load_data(\n",
    "    name: str,\n",
    "    conds: Dict[str, Union[Iterable[str], str]] = None\n",
    ") -> Optional[pd.DataFrame]:\n",
    "    paths = [\n",
    "        (d, os.path.join('data', d, f'{name}.csv'))\n",
    "        for d in os.listdir('data')\n",
    "        if d.startswith('P')\n",
    "    ]\n",
    "    data = pd.concat([\n",
    "        pd.read_csv(p).assign(pcode=pcode)\n",
    "        for pcode, p in paths\n",
    "        if os.path.exists(p)\n",
    "    ], ignore_index=True)\n",
    "    \n",
    "    if conds is not None:\n",
    "        for k, v in conds.items():\n",
    "            if type(v) is str:\n",
    "                data = data.loc[lambda x: x[k] == v, :]\n",
    "            else:\n",
    "                data = data.loc[lambda x: x[k].isin(v), :]\n",
    "                \n",
    "    data = data.assign(\n",
    "        timestamp=lambda x: pd.to_datetime(x['timestamp'], unit='ms', utc=True).dt.tz_convert(DEFAULT_TZ)\n",
    "    ).set_index(\n",
    "        ['pcode', 'timestamp']\n",
    "    )\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def _load_app_usage() -> Dict[str, pd.Series]:\n",
    "    data = _load_data(name='AppUsageEvent', conds={\n",
    "        'type': ['MOVE_TO_FOREGROUND', 'MOVE_TO_BACKGROUND']\n",
    "    }).assign(\n",
    "        raw=lambda x: np.where(x['type'] == 'MOVE_TO_FOREGROUND', x['packageName'], None),\n",
    "        #cls=lambda x: np.where(x['type'] == 'MOVE_TO_FOREGROUND', x['packageName'].replace(category), None),\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'PHO#APP_RAW': data['raw'].astype('object'),\n",
    "        'PHO#APP_CLS': data['category'].astype('object')\n",
    "    }\n",
    "\n",
    "\n",
    "def _load_connectivity() -> Dict[str, pd.Series]:\n",
    "    data = _load_data(name='Connectivity').assign(\n",
    "        type=lambda x: np.where(x['isConnected'] == True, x['type'], 'DISCONNECTED')\n",
    "    ).replace({\n",
    "        'type': {\n",
    "            'MOBILE': 'MOBILE',\n",
    "            'MOBILE_DUN': 'MOBILE',\n",
    "            'UNDEFINED': 'DISCONNECTED',\n",
    "            'VPN': 'MOBILE',\n",
    "            'WIFI': 'WIFI'\n",
    "        }\n",
    "    })\n",
    "\n",
    "    return {\n",
    "        'PHO#CON': data['type'].astype('object')\n",
    "    }\n",
    "\n",
    "\n",
    "def _load_battery() -> Dict[str, pd.Series]:\n",
    "    data = _load_data(name='BatteryEvent').assign(\n",
    "        temperature=lambda x: (x['temperature'] / 10.0).astype(np.float64)\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'PHO#BAT_LEV': data['level'].astype('float64'),\n",
    "        'PHO#BAT_CHG': data['status'].astype('object'),\n",
    "        'PHO#BAT_TMP': data['temperature'].astype('float64')\n",
    "    }\n",
    "        \n",
    "\n",
    "def _load_call() -> Dict[str, pd.Series]:\n",
    "    data = _load_data(name='CallEvent', conds={\n",
    "        'type': ['OUTGOING', 'INCOMING']\n",
    "    }).reset_index()\n",
    "        \n",
    "    new_data = []\n",
    "        \n",
    "    for pcode in data['pcode'].unique():\n",
    "        sub = data.loc[\n",
    "            lambda x: x['pcode'] == pcode, :\n",
    "        ].sort_values(\n",
    "            'timestamp'\n",
    "        )\n",
    "        for row in sub.itertuples():\n",
    "            new_data.append({\n",
    "                'pcode': row.pcode,\n",
    "                'timestamp': row.timestamp,                \n",
    "                'type': row.type,\n",
    "            })\n",
    "            new_data.append({\n",
    "                'pcode': row.pcode,\n",
    "                'timestamp': row.timestamp + timedelta(seconds=row.duration),\n",
    "                'type': 'IDLE'\n",
    "            })\n",
    "    new_data = pd.DataFrame(new_data).set_index(\n",
    "        ['pcode', 'timestamp']\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'PHO#CAL': new_data['type'].astype('object'),\n",
    "    }\n",
    "\n",
    "\n",
    "def _load_data_traffic() -> Dict[str, pd.Series]:\n",
    "    data = _load_data(name='DataTraffic')\n",
    "\n",
    "    return {\n",
    "        'PHO#DAT_RCV': data['rxKiloBytes'].astype('float64'),\n",
    "        'PHO#DAT_SNT': data['txKiloBytes'].astype('float64')\n",
    "    }\n",
    "\n",
    "\n",
    "def _load_device_event() -> Dict[str, pd.Series]:\n",
    "    data = _load_data(name='DeviceEvent')\n",
    "    \n",
    "    scr = data.loc[\n",
    "        lambda x: x['type'].isin(['SCREEN_ON', 'SCREEN_OFF', 'UNLOCK']), :\n",
    "    ].assign(\n",
    "        type=lambda x: x['type'].str.replace('SCREEN_', '')\n",
    "    )\n",
    "    \n",
    "    rng = data.loc[\n",
    "        lambda x: x['type'].isin(['RINGER_MODE_VIBRATE', 'RINGER_MODE_SILENT', 'RINGER_MODE_NORMAL']), :\n",
    "    ].assign(\n",
    "        type=lambda x: x['type'].str.replace('RINGER_MODE_', '')\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'PHO#SCR': scr['type'].astype('object'),\n",
    "        'PHO#RNG': rng['type'].astype('object'),\n",
    "    }\n",
    "\n",
    "\n",
    "def _load_location() -> Dict[str, pd.Series]:\n",
    "    def _haversine(_lat1: float, _lat2: float, _lng1: float, _lng2: float) -> float:\n",
    "        if np.isnan(_lat1) or np.isnan(_lat2) or np.isnan(_lng1) or np.isnan(_lng2):\n",
    "            return 0.0\n",
    "\n",
    "        _lat1_r, _lat2_r, _lng1_r, _lng2_r = np.radians(_lat1), np.radians(_lat2), np.radians(_lng1), np.radians(_lng2)\n",
    "        _lat = _lat2_r - _lat1_r\n",
    "        _lng = _lng2_r - _lng1_r\n",
    "        _R = 6371008.8\n",
    "        _d = np.sin(_lat * 0.5) ** 2 + np.cos(_lat1_r) * np.cos(_lat2_r) * np.sin(_lng * 0.5) ** 2\n",
    "        return 2 * _R * np.arcsin(np.sqrt(_d))\n",
    "\n",
    "    data = _load_data(name='Location').reset_index()\n",
    "    new_data = []\n",
    "\n",
    "    for pcode in data['pcode'].unique():\n",
    "        sub = data.loc[\n",
    "            lambda x: x['pcode'] == pcode, :\n",
    "        ].sort_values(\n",
    "            'timestamp'\n",
    "        )\n",
    "\n",
    "        new_sub = pd.concat([\n",
    "            sub, sub.rename(lambda x: f'_{x}', axis=1).shift(-1)\n",
    "        ], axis=1).assign(\n",
    "            dist=lambda x: [\n",
    "                _haversine(*_coord) for _coord\n",
    "                in zip(x['latitude'], x['_latitude'], x['longitude'], x['_longitude'])\n",
    "            ],\n",
    "            cluster=lambda x: [\n",
    "                geo.encode(lat, lon, precision=6)\n",
    "                for lat, lon in zip(x['latitude'], x['longitude'])\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        for row in new_sub.itertuples():\n",
    "            new_data.append({\n",
    "                'pcode': row.pcode,\n",
    "                'timestamp': row.timestamp,                \n",
    "                'dist': row.dist,\n",
    "                'cluster': row.cluster,\n",
    "            })\n",
    "\n",
    "    new_data = pd.DataFrame(new_data).set_index(\n",
    "        ['pcode', 'timestamp']\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'PHO#LOC_CLS': new_data['cluster'].astype('object'),\n",
    "        'PHO#LOC_DST': new_data['dist'].astype('float64'),\n",
    "    }\n",
    "\n",
    "\n",
    "def _load_activity() -> Dict[str, pd.Series]:\n",
    "    data = _load_data(name='ActivityTransition', conds={\n",
    "        'transitionType': ['ENTER_WALKING', 'ENTER_STILL', 'ENTER_IN_VEHICLE', 'ENTER_ON_BICYCLE', 'ENTER_RUNNING']\n",
    "    }).assign(\n",
    "        type=lambda x: x['transitionType'].str.replace('ENTER_', '')\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'PHO#ACT': data['type'].astype('object')\n",
    "    }\n",
    "\n",
    "\n",
    "def _load_accelerometer() -> Dict[str, pd.Series]:\n",
    "    data = _load_data(name='Acceleration')\n",
    "    return {\n",
    "        'WEA#ACC_X': data['X'].astype('float64'),\n",
    "        'WEA#ACC_Y': data['Y'].astype('float64'),\n",
    "        'WEA#ACC_Z': data['Z'].astype('float64')        \n",
    "    }\n",
    "\n",
    "\n",
    "def _load_ultra_violet() -> Dict[str, pd.Series]:\n",
    "    data = _load_data(name='UltraViolet')\n",
    "    return {\n",
    "        'WEA#ULB_IDX': data['UVIndexLevel'].astype('object'),\n",
    "    }\n",
    "\n",
    "\n",
    "def _load_skin_temperature() -> Dict[str, pd.Series]:\n",
    "    data = _load_data(name='SkinTemperature')\n",
    "    return {\n",
    "        'WEA#TMP': data['Temperature'].astype('float64'),\n",
    "    }\n",
    "\n",
    "\n",
    "def _load_rr_interval() -> Dict[str, pd.Series]:\n",
    "    data = _load_data(name='RRI')\n",
    "    return {\n",
    "        'WEA#RRI': data['Interval'].astype('float64'),\n",
    "    }\n",
    "\n",
    "\n",
    "def _load_brightness() -> Dict[str, pd.Series]:\n",
    "    data = _load_data(name='AmbientLight')\n",
    "    return {\n",
    "        'WEA#BRI': data['Brightness'].astype('float64'),\n",
    "    }\n",
    "\n",
    "\n",
    "def _load_step_counts() -> Dict[str, pd.Series]:\n",
    "    data = _load_data(name='StepCount').reset_index()\n",
    "    new_data = []\n",
    "\n",
    "    for pcode in data['pcode'].unique():\n",
    "        sub = data.loc[\n",
    "            lambda x: x['pcode'] == pcode, :\n",
    "        ].sort_values(\n",
    "            'timestamp'\n",
    "        )\n",
    "\n",
    "        new_sub = pd.concat([\n",
    "            sub, sub.rename(lambda x: f'_{x}', axis=1).shift(-1)\n",
    "        ], axis=1).assign(\n",
    "            diff=lambda x: x['_TotalSteps'] - x['TotalSteps']\n",
    "        )\n",
    "\n",
    "        for row in new_sub.itertuples():\n",
    "            new_data.append({\n",
    "                'pcode': row.pcode,\n",
    "                'timestamp': row.timestamp,                \n",
    "                'diff': row.diff\n",
    "            })\n",
    "\n",
    "    new_data = pd.DataFrame(new_data).set_index(\n",
    "        ['pcode', 'timestamp']\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'WEA#STP': new_data['diff'].astype('float64'),\n",
    "    }\n",
    "\n",
    "\n",
    "def _load_heart_rate() -> Dict[str, pd.Series]:\n",
    "    data = _load_data(name='HR')\n",
    "    return {\n",
    "        'WEA#HRT': data['BPM'].astype('float64')\n",
    "    }\n",
    "\n",
    "\n",
    "def _load_gsr() -> Dict[str, pd.Series]:\n",
    "    data = _load_data(name='EDA')\n",
    "    return {\n",
    "        'WEA#GSR': data['Resistance'].astype('float64')\n",
    "    }\n",
    "\n",
    "\n",
    "def _load_distance() -> Dict[str, pd.Series]:\n",
    "    data = _load_data(name='Distance').reset_index()\n",
    "    new_data = []\n",
    "\n",
    "    for pcode in data['pcode'].unique():\n",
    "        sub = data.loc[\n",
    "            lambda x: x['pcode'] == pcode, :\n",
    "        ].sort_values(\n",
    "            'timestamp'\n",
    "        )\n",
    "\n",
    "        new_sub = pd.concat([\n",
    "            sub, sub.rename(lambda x: f'_{x}', axis=1).shift(-1)\n",
    "        ], axis=1).assign(\n",
    "            diff=lambda x: x['_TotalDistance'] - x['TotalDistance']\n",
    "        )\n",
    "\n",
    "        for row in new_sub.itertuples():\n",
    "            new_data.append({\n",
    "                'pcode': row.pcode,\n",
    "                'timestamp': row.timestamp,                \n",
    "                'diff': row.diff,\n",
    "                'motion': row.MotionType,\n",
    "                'pace': row.Pace,\n",
    "                'speed': row.Speed,\n",
    "            })\n",
    "\n",
    "    new_data = pd.DataFrame(new_data).set_index(\n",
    "        ['pcode', 'timestamp']\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'WEA#ACT_DST': new_data['diff'].astype('float64'),\n",
    "        'WEA#ACT_MOT': new_data['motion'].astype('object'),\n",
    "        'WEA#ACT_PAC': new_data['pace'].astype('float64'),\n",
    "        'WEA#ACT_SPD': new_data['speed'].astype('float64'),\n",
    "    }\n",
    "\n",
    "\n",
    "def _load_calories() -> Dict[str, pd.Series]:\n",
    "    data = _load_data(name='Calorie').reset_index()\n",
    "    new_data = []\n",
    "\n",
    "    for pcode in data['pcode'].unique():\n",
    "        sub = data.loc[\n",
    "            lambda x: x['pcode'] == pcode, :\n",
    "        ].sort_values(\n",
    "            'timestamp'\n",
    "        )\n",
    "\n",
    "        new_sub = pd.concat([\n",
    "            sub, sub.rename(lambda x: f'_{x}', axis=1).shift(-1)\n",
    "        ], axis=1).assign(\n",
    "            diff=lambda x: x['_TotalCalories'] - x['TotalCalories']\n",
    "        )\n",
    "\n",
    "        for row in new_sub.itertuples():\n",
    "            new_data.append({\n",
    "                'pcode': row.pcode,\n",
    "                'timestamp': row.timestamp,                \n",
    "                'diff': row.diff\n",
    "            })\n",
    "\n",
    "    new_data = pd.DataFrame(new_data).set_index(\n",
    "        ['pcode', 'timestamp']\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'WEA#CAL': new_data['diff'].astype('float64')       \n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback as tb\n",
    "from functools import reduce\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "DATA = defaultdict(dict)\n",
    "\n",
    "func_loads = [\n",
    "    _load_app_usage,\n",
    "    _load_connectivity,\n",
    "    _load_battery,\n",
    "    _load_call,\n",
    "    _load_data_traffic,\n",
    "    _load_device_event,\n",
    "    _load_location,\n",
    "    _load_activity,\n",
    "    _load_accelerometer,\n",
    "    _load_ultra_violet,\n",
    "    _load_skin_temperature,\n",
    "    _load_rr_interval,\n",
    "    _load_brightness,\n",
    "    _load_step_counts,\n",
    "    _load_heart_rate,\n",
    "    _load_gsr,\n",
    "    _load_distance,\n",
    "    _load_calories\n",
    "]\n",
    "\n",
    "data = []\n",
    "\n",
    "for func in func_loads:\n",
    "    try:\n",
    "        d = func()\n",
    "    except:\n",
    "        d = dict()\n",
    "        log(f'Error occurs in data = {func.__name__}. Caused by: {tb.format_exc()}')\n",
    "    finally:\n",
    "        data.append(d)\n",
    "\n",
    "data = reduce(lambda a, b: {**a, **b}, data)\n",
    "for k, v in data.items():\n",
    "    DATA[k] = v.sort_index()\n",
    "\n",
    "for pcode in LABEL.index.droplevel('timestamp').unique():\n",
    "    data_subject = dict()\n",
    "    \n",
    "    for k, v in DATA.items():\n",
    "        try:\n",
    "            data_subject[k] = v[pcode]\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    dump(data_subject, f'./proc/raw_data-{pcode}.pkl')\n",
    "\n",
    "del DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensor data from Smartphone and MS Band2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta as td\n",
    "\n",
    "LABEL = load('./proc/label.pkl')\n",
    "\n",
    "EXP_DATE = LABEL.reset_index().groupby(\n",
    "    ['pcode']\n",
    ").agg(\n",
    "    start_day=pd.NamedAgg(column='timestamp', aggfunc=lambda x: x.min().replace(hour=0, minute=0, second=0, microsecond=0)),\n",
    "    end_day=pd.NamedAgg(column='timestamp', aggfunc=lambda x: (x.max() + td(days=1)).replace(hour=0, minute=0, second=0, microsecond=0)),\n",
    ")\n",
    "\n",
    "DIST_DATA = []\n",
    "\n",
    "for p in EXP_DATE.index:\n",
    "    raw_data = load(f'./proc/raw_data-{p}.pkl')\n",
    "\n",
    "    dist_temp = []\n",
    "\n",
    "    for data_type, data_values in raw_data.items():\n",
    "        data_sub = pd.concat([\n",
    "            data_values.to_frame().reset_index().assign(pcode=p)\n",
    "        ]).merge(\n",
    "            EXP_DATE, on='pcode', how='left'\n",
    "        ).loc[\n",
    "            lambda x: ((x['timestamp'] - x['start_day']).dt.total_seconds() >= 0) & ((x['end_day'] - x['timestamp']).dt.total_seconds() >= 0), :\n",
    "        ].assign(\n",
    "            day = lambda x: np.floor((x['timestamp'] - x['start_day']).dt.total_seconds() / 60 / 60 / 24)\n",
    "        ).groupby(\n",
    "            ['pcode', 'day']\n",
    "        ).count()['timestamp'].reset_index().assign(\n",
    "            type=data_type\n",
    "        )\n",
    "\n",
    "        dist_temp.append(data_sub)\n",
    "\n",
    "    DIST_DATA.append(pd.concat(dist_temp))\n",
    "    \n",
    "DIST_DATA = pd.concat(DIST_DATA)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIST_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIST_DATA.groupby(\n",
    "        ['type', 'pcode']\n",
    "    ).sum()['timestamp'].reset_index().groupby(\n",
    "        'type'\n",
    "    )['timestamp'].agg(\n",
    "        lambda x: 'N={0:.2f}; mean={1[0]:.2f} (SD: {1[1]:.2f}; [{1[2]:.2f}, {1[3]:.2f}]); med={1[4]:.2f} ([{1[5]:.2f}, {1[6]:.2f}])'.format(\n",
    "            np.sum(x), stat(x)\n",
    "        )\n",
    "    ).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "\n",
    "alt.data_transformers.disable_max_rows()\n",
    "\n",
    "selection = alt.selection_single(\n",
    "    fields=['type'],\n",
    "    bind=alt.binding_select(\n",
    "        options=DIST_DATA['type'].unique(),\n",
    "        name='Data Type: '\n",
    "    )\n",
    ")\n",
    "\n",
    "alt.Chart(DIST_DATA).mark_rect().encode(\n",
    "     x=alt.X('day:O', title='Time (days)'),\n",
    "     y=alt.Y('pcode:O', title='Pcode'),\n",
    "     color=alt.Color('timestamp:Q', title='Count')\n",
    ").transform_filter(\n",
    "    selection\n",
    ").add_selection(\n",
    "    selection\n",
    ").properties(\n",
    "    title='# Instances',\n",
    "    width=300,\n",
    "    height=800\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, PHO#CAL (Call log) and PHO#RNG (Ringer mode) were not regularly collected (i.e., too many missing values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "EXCLUDE_DTYPE = ['PHO#CAL', 'PHO#RNG']\n",
    "LABEL = load('./proc/label.pkl')\n",
    "\n",
    "for pcode in LABEL.index.droplevel('timestamp').unique():\n",
    "    data = load(f'./proc/raw_data-{pcode}.pkl')\n",
    "    data = {\n",
    "        k: v\n",
    "        for k, v in data.items()\n",
    "        if k not in EXCLUDE_DTYPE\n",
    "    }\n",
    "    dump(data, f'./proc/clean_data-{pcode}.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, Callable, Union, Tuple, List, Optional\n",
    "from datetime import timedelta as td\n",
    "from scipy import stats\n",
    "import time\n",
    "\n",
    "\n",
    "def extract(\n",
    "        pid: str,\n",
    "        data: Dict[str, pd.Series],\n",
    "        label: pd.Series,        \n",
    "        window_data: Dict[str, Union[int, Callable[[pd.Timestamp], int]]],\n",
    "        window_label: Dict[str, Union[int, Callable[[pd.Timestamp], int]]],        \n",
    "        categories: Dict[str, Optional[List[any]]] = None,\n",
    "        pinfo: Dict[str, any] = None,\n",
    "        resample_s: int = -1,\n",
    "        verbosity: int = -1\n",
    ") -> Tuple[pd.DataFrame, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    def _log(_msg, _verb):\n",
    "        _prefix = '[DEBUG]' if _verb == 0 else '[INFO]'\n",
    "        if _verb <= verbosity:\n",
    "            log(f'{_prefix} {_msg}')\n",
    "\n",
    "    elapsed_time = time.time()\n",
    "    _log(f'Start feature extraction for pid == {pid}', 0)\n",
    "    \n",
    "    categories = categories or dict()\n",
    "    pinfo = pinfo or dict()\n",
    "    data_resampled = dict()\n",
    "    \n",
    "    for k, v in data.items():\n",
    "        v = v.loc[lambda x: ~x.index.duplicated(keep=False)]\n",
    "        if k in categories:\n",
    "            data_resampled[k] = v.resample(f'{resample_s}S').ffill().dropna() if resample_s > 0 and len(v) > 0 else v\n",
    "        else:\n",
    "            data_resampled[k] = v.resample(f'{resample_s}S').mean().interpolate(method='linear').dropna() if resample_s > 0 and len(v) > 0 else v\n",
    "    _log(f'Data resampled', 1)\n",
    "\n",
    "    X, y, group, date_times = [], [], [], []\n",
    "\n",
    "    for t_label, v_label in zip(label.index, label.values):\n",
    "        row = dict()\n",
    "\n",
    "        # Features relevant to participants' info\n",
    "        for d_prefix, d_value in pinfo.items():\n",
    "            row[f'PIF#{d_prefix}'] = d_value\n",
    "        _log(f'Extracted feature relevant to personal info at {t_label}', 1)\n",
    "\n",
    "        # Features relevant to time\n",
    "        day_of_week = ['MON', 'TUE', 'WED', 'THU', 'FRI', 'SAT', 'SUN'][t_label.isoweekday() - 1]\n",
    "        is_weekend = 'Y' if t_label.isoweekday() > 5 else 'N'\n",
    "        hour = t_label.hour\n",
    "\n",
    "        if 6 <= hour < 9:\n",
    "            hour_name = 'DAWN'\n",
    "        elif 9 <= hour < 12:\n",
    "            hour_name = 'MORNING'\n",
    "        elif 12 <= hour < 15:\n",
    "            hour_name = 'AFTERNOON'\n",
    "        elif 15 <= hour < 18:\n",
    "            hour_name = 'LATE_AFTERNOON'\n",
    "        elif 18 <= hour < 21:\n",
    "            hour_name = 'EVENING'\n",
    "        elif 21 <= hour < 24:\n",
    "            hour_name = 'NIGHT'\n",
    "        else:\n",
    "            hour_name = 'MIDNIGHT'\n",
    "        \n",
    "        row['TIM#DOW'] = day_of_week\n",
    "        row['TIM#WKD'] = is_weekend\n",
    "        row['TIM#HRN'] = hour_name\n",
    "        _log(f'Extracted feature relevant to delivery time at {t_label}', 1)\n",
    "\n",
    "        # Features relevant to latest value of a given smartphone data\n",
    "        for d_prefix, d_value in data_resampled.items():\n",
    "            if d_prefix in categories and categories[d_prefix] is None:\n",
    "                continue\n",
    "                \n",
    "            d = d_value.sort_index().loc[:t_label]\n",
    "            v = None\n",
    "            if len(d):\n",
    "                v = d.iloc[-1]\n",
    "            else:\n",
    "                if d.dtype == object:\n",
    "                    v = 'UNKNOWN'\n",
    "                else:\n",
    "                    v = 0.0\n",
    "            row[f'{d_prefix}#VAL'] = v\n",
    "        _log(f'Extracted feature relevant to latest smartphone value at {t_label}', 1)\n",
    "\n",
    "        # Features relevant to duration since the latest state change (only for categorical smartphone data)\n",
    "        for d_prefix, d_value in data_resampled.items():\n",
    "            if d_prefix not in categories:\n",
    "                continue\n",
    "\n",
    "            d = d_value.sort_index().loc[:t_label]\n",
    "            row[f'{d_prefix}#DLC'] = (t_label - d.loc[lambda x: x == x.iloc[-1]].index.min()).total_seconds() if len(d) else -1\n",
    "\n",
    "            cats = categories[d_prefix] or list()\n",
    "            for c in cats:\n",
    "                d = d_value.sort_index().loc[:t_label].loc[lambda x: x == c]\n",
    "                row[f'{d_prefix}#DLC={c}'] = (t_label - d.index[-1]).total_seconds() if len(d) else -1\n",
    "\n",
    "            _log(f'Extracted feature relevant to duration since the latest change at {t_label} for {d_prefix}', 1)\n",
    "\n",
    "        # Features extracted from windowed smartphone data\n",
    "        for w_suffix, w_value in window_data.items():\n",
    "            w_value = w_value(t_label) if isinstance(w_value, Callable) else w_value\n",
    "\n",
    "            for d_prefix, d_value in data_resampled.items():\n",
    "                d = d_value.loc[(t_label - td(seconds=w_value)):t_label]\n",
    "\n",
    "                if len(d) == 0:\n",
    "                    continue\n",
    "                \n",
    "                if d_prefix in categories:\n",
    "                    cnt = d.value_counts()\n",
    "                    val, sup = cnt.index, cnt.values\n",
    "                    hist = {k: v for k, v in zip(val, sup)}\n",
    "\n",
    "                    # Information Entropy\n",
    "                    row[f'{d_prefix}#ETP#{w_suffix}'] = stats.entropy(sup / len(d)) \n",
    "                    \n",
    "                    # Abs. Sum of Changes\n",
    "                    row[f'{d_prefix}#ASC#{w_suffix}'] = np.sum(d.values[1:] != d.values[:-1]) \n",
    "\n",
    "                    cats = categories[d_prefix] or list()\n",
    "                    \n",
    "                    # Duration\n",
    "                    for c in cats:\n",
    "                        row[f'{d_prefix}#DUR={c}#{w_suffix}'] = hist[c] if c in hist else 0\n",
    "\n",
    "                else:\n",
    "                    d = d.astype('float32').values\n",
    "                    hist, _ = np.histogram(d, bins='doane', density=False)\n",
    "                    std = np.sqrt(np.var(d, ddof=1)) if len(d) > 1 else 0\n",
    "                    norm_x = (d - np.mean(d)) / std if std != 0 else np.zeros(len(d))\n",
    "\n",
    "                    row[f'{d_prefix}#AVG#{w_suffix}'] = np.mean(d) # Sample mean\n",
    "                    row[f'{d_prefix}#STD#{w_suffix}'] = std # Sample standard deviation\n",
    "                    row[f'{d_prefix}#SKW#{w_suffix}'] = stats.skew(d, bias=False) # Sample skewness\n",
    "                    row[f'{d_prefix}#KUR#{w_suffix}'] = stats.kurtosis(d, bias=False) # Sample kurtosis\n",
    "                    row[f'{d_prefix}#ASC#{w_suffix}'] = np.sum(np.abs(np.diff(d))) # Abstract sum of changes\n",
    "                    row[f'{d_prefix}#BEP#{w_suffix}'] = stats.entropy(hist) # Binned entropy\n",
    "                    row[f'{d_prefix}#MED#{w_suffix}'] = np.median(d) # Median\n",
    "                    row[f'{d_prefix}#TSC#{w_suffix}'] = np.sqrt(np.sum(np.power(np.diff(norm_x), 2))) # Timeseries complexity\n",
    "                \n",
    "                _log(f'Extracted feature relevant to a time-window data at {t_label} for {d_prefix} and {w_suffix}', 1)\n",
    "    \n",
    "        # Features extracted from previous respones behavior toward JIT intervention\n",
    "        for w_suffix, w_value in window_label.items():\n",
    "            w_value = w_value(t_label) if isinstance(w_value, Callable) else w_value\n",
    "            d = label.loc[(t_label - td(seconds=w_value)):t_label]\n",
    "            row[f'REC#LIK#{w_suffix}'] = np.sum(d == 1.0) / len(d) if len(d) > 0 else 0\n",
    "            _log(f'Extracted feature relevant to a time-window resp. behavior at {t_label} for {w_suffix}', 1)\n",
    "\n",
    "        X.append(row)\n",
    "        y.append(v_label)\n",
    "        group.append(pid)\n",
    "        date_times.append(t_label)\n",
    "\n",
    "    X, y, group, date_times = pd.DataFrame(X), np.asarray(y), np.asarray(group), np.asarray(date_times)\n",
    "    t_s = date_times.min().replace(hour=0, minute=0, second=0, microsecond=0).timestamp()\n",
    "    timestamps = np.asarray(list(map(lambda x: x.timestamp() - t_s, date_times)))\n",
    "   \n",
    "    dtypes = {\n",
    "        k: 'object' if v == object else 'float32'\n",
    "        for k, v in X.dtypes.items()\n",
    "    }\n",
    "    C = X.columns\n",
    "    C_cat = list(sorted([c for c in C if dtypes[c] == object]))\n",
    "    C_num = list(sorted([c for c in C if dtypes[c] != object]))\n",
    "    X_C, X_N = X[C_cat], X[C_num]\n",
    "    X_C = X_C.fillna(value='UNKNOWN')\n",
    "    X_N = X_N.fillna(value=0.0)\n",
    "    X = pd.DataFrame(np.concatenate([X_C, X_N], axis=1), columns=C_cat + C_num).astype(dtypes)\n",
    "    \n",
    "    _log(f'Completed feature extraction for pid == {pid} ({time.time() - elapsed_time} s)', 0)\n",
    "    return X, y, group, timestamps, date_times\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution with *ray* module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ray\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "SUBJECTS = pd.read_csv('./data/UserInfo.csv')[[\n",
    "    'Pcode', 'Age', 'Gender', 'Openness', 'Conscientiousness', \n",
    "    'Neuroticism', 'Extraversion', 'Agreeableness', 'PSS10', 'PHQ9', 'GHQ12'\n",
    "]].rename(columns={\n",
    "    'Age': 'AGE', 'Gender': 'GEN', \n",
    "    'Openness': 'BFI_OPN', 'Conscientiousness': 'BFI_CON', \n",
    "    'Neuroticism': 'BFI_NEU', 'Extraversion': 'BFI_EXT',\n",
    "    'Agreeableness': 'BFI_AGR', \n",
    "    'PSS10': 'PSS', 'PHQ9': 'PHQ', 'GHQ12': 'GHQ'\n",
    "}).set_index(\n",
    "    'Pcode'\n",
    ").to_dict('index')\n",
    "\n",
    "LABEL = load('./proc/label.pkl')\n",
    "\n",
    "PCODES = LABEL.index.droplevel('timestamp').unique()\n",
    "\n",
    "WIN_DATA = {\n",
    "    'M1': 60,\n",
    "    'M5': 60 * 5,\n",
    "    'M10': 60 * 10,\n",
    "    'M30': 60 * 30,\n",
    "    'H1': 60 * 60,\n",
    "    'H3': 60 * 60 * 3,\n",
    "    'H6': 60 * 60 * 6,\n",
    "    'D1': 60 * 60 * 24\n",
    "}\n",
    "\n",
    "WIN_LABEL = {\n",
    "    'H3': 60 * 60 * 3,\n",
    "    'H6': 60 * 60 * 6,\n",
    "    'H12': 60 * 60 * 12,\n",
    "    'D1': 60 * 60 * 24,\n",
    "    'D3': 60 * 60 * 24 * 3\n",
    "}\n",
    "\n",
    "\n",
    "CATEGORIES = defaultdict(list)\n",
    "CATEGORIES['PHO#APP_RAW'] = None\n",
    "CATEGORIES['PHO#LOC_CLS'] = None\n",
    "\n",
    "for pcode in PCODES:\n",
    "    d = load(f'./proc/clean_data-{pcode}.pkl')\n",
    "    \n",
    "    for c in [\n",
    "        'PHO#APP_CLS', 'PHO#CON', 'PHO#BAT_CHG', 'PHO#SCR', 'PHO#ACT', 'WEA#ULB_IDX', 'WEA#ACT_MOT'\n",
    "    ]:\n",
    "        CATEGORIES[c].extend(list(filter(lambda x: x is not None, d[c].unique())))\n",
    "\n",
    "CATEGORIES = {\n",
    "    k: None if v is None else list(set(v))\n",
    "    for k, v in CATEGORIES.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with on_ray(ignore_reinit_error=True, num_cpus=7):\n",
    "    func = ray.remote(extract).remote\n",
    "\n",
    "    for l in LABEL.columns:\n",
    "        jobs = []\n",
    "        \n",
    "        for pcode in PCODES:\n",
    "            pinfo = SUBJECTS[pcode]\n",
    "            data = load(f'./proc/clean_data-{pcode}.pkl')\n",
    "            label = LABEL[l].loc[pcode]\n",
    "            \n",
    "            job = func(\n",
    "                pid=pcode,\n",
    "                data=data,\n",
    "                label=label,\n",
    "                window_data=WIN_DATA,\n",
    "                window_label=WIN_LABEL,\n",
    "                categories=CATEGORIES,\n",
    "                pinfo=pinfo,\n",
    "                resample_s=1,\n",
    "                verbosity=0\n",
    "            )\n",
    "            jobs.append(job)\n",
    "            \n",
    "        jobs = ray.get(jobs)\n",
    "\n",
    "        X = pd.concat(list(map(lambda x: x[0], jobs)), ignore_index=True)\n",
    "        y = np.concatenate(list(map(lambda x: x[1], jobs)), axis=0)\n",
    "        group = np.concatenate(list(map(lambda x: x[2], jobs)), axis=0)\n",
    "        t = np.concatenate(list(map(lambda x: x[3], jobs)), axis=0)\n",
    "        date_times = np.concatenate(list(map(lambda x: x[4], jobs)), axis=0)\n",
    "        \n",
    "        dataset = dict(\n",
    "            X=X,\n",
    "            y=y,\n",
    "            group=group,\n",
    "            t=t,\n",
    "            date_times=date_times\n",
    "        )\n",
    "        dump(dataset, f'./proc/dataset.{l}.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import altair as alt\n",
    "from typing import Iterable\n",
    "\n",
    "\n",
    "def vis_feature(\n",
    "    X: pd.DataFrame,\n",
    "    y: np.ndarray,\n",
    "    pids: np.ndarray,    \n",
    "    feature: str\n",
    "): \n",
    "    data = X.assign(\n",
    "        label = y,\n",
    "        pids = pids\n",
    "    )[[feature, 'label', 'pids']]\n",
    "        \n",
    "    dtype = data[feature].dtype\n",
    "    \n",
    "    if dtype == object:\n",
    "        data_all = data.assign(\n",
    "            cnt=0\n",
    "        ).groupby(\n",
    "            ['label', feature], as_index=False\n",
    "        ).count()\n",
    "        \n",
    "        data_sub = data.assign(\n",
    "            cnt=0\n",
    "        ).groupby(\n",
    "            ['label', 'pids', feature], as_index=False\n",
    "        ).count()\n",
    "        \n",
    "        c_all = alt.Chart(\n",
    "            data_all\n",
    "        ).encode(\n",
    "            x=alt.X('label:N', title='Class'),\n",
    "            y=alt.Y('cnt:Q', title='Ratio', stack='normalize'),\n",
    "            color=f'{feature}:N'\n",
    "        ).mark_bar().properties(\n",
    "            width=30,\n",
    "            height=200,\n",
    "        )\n",
    "        \n",
    "        c_sub = alt.Chart(\n",
    "            data_sub\n",
    "        ).encode(\n",
    "            x=alt.X('label:N', title='Class'),\n",
    "            y=alt.Y('cnt:Q', title='Ratio', stack='normalize'),\n",
    "            color=f'{feature}:N'                    \n",
    "        ).mark_bar().properties(\n",
    "            width=30,\n",
    "            height=200\n",
    "        ).facet(\n",
    "            column=alt.Column('pids:N', title='PIDs')\n",
    "        )\n",
    "        \n",
    "        return c_all | c_sub\n",
    "        \n",
    "    else:\n",
    "        c_all = alt.Chart(\n",
    "            data\n",
    "        ).encode(\n",
    "            x=alt.X('label:N', title='Class'),\n",
    "            y=alt.Y(f'{feature}:Q', title='Value'),\n",
    "            color=alt.Color('label:N')\n",
    "        ).mark_boxplot(size=10).properties(\n",
    "            width=30,\n",
    "            height=200\n",
    "        )\n",
    "        \n",
    "        c_sub = alt.Chart(\n",
    "            data\n",
    "        ).encode(\n",
    "            x=alt.X('label:N', title='Class'),\n",
    "            y=alt.Y(f'{feature}:Q', title='Value'),\n",
    "            color=alt.Color('label:N', title='Class')\n",
    "        ).mark_boxplot(size=10).properties(\n",
    "            width=30,\n",
    "            height=200,\n",
    "        ).facet(\n",
    "            column=alt.Column('pids:N', title='PIDs')\n",
    "        )\n",
    "        \n",
    "        return c_all | c_sub\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arousal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact, fixed, widgets\n",
    "\n",
    "\n",
    "DATA_ARO_FIX = load('./proc/dataset.aro_fix.pkl')\n",
    "\n",
    "interact(\n",
    "    vis_feature,\n",
    "    X=fixed(DATA_ARO_FIX['X']),\n",
    "    y=fixed(DATA_ARO_FIX['y']),\n",
    "    pids=fixed(DATA_ARO_FIX['group']),\n",
    "    feature=widgets.Select(options=sorted(DATA_ARO_FIX['X'].columns), rows=15)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, fixed, widgets\n",
    "\n",
    "\n",
    "DATA_VAL_DYN = load('./proc/dataset.val_fix.pkl')\n",
    "\n",
    "interact(\n",
    "    vis_feature,\n",
    "    X=fixed(DATA_VAL_DYN['X']),\n",
    "    y=fixed(DATA_VAL_DYN['y']),\n",
    "    pids=fixed(DATA_VAL_DYN['group']),\n",
    "    feature=widgets.Select(options=sorted(DATA_VAL_DYN['X'].columns), rows=15)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, fixed, widgets\n",
    "\n",
    "\n",
    "DATA_STS_FIX = load('./proc/dataset.sts_fix.pkl')\n",
    "\n",
    "interact(\n",
    "    vis_feature,\n",
    "    X=fixed(DATA_STS_FIX['X']),\n",
    "    y=fixed(DATA_STS_FIX['y']),\n",
    "    pids=fixed(DATA_STS_FIX['group']),\n",
    "    feature=widgets.Select(options=sorted(DATA_STS_FIX['X'].columns), rows=15)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disturbance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, fixed, widgets\n",
    "\n",
    "\n",
    "DATA_DST_FIX = load('./proc/dataset.dst_fix.pkl')\n",
    "\n",
    "interact(\n",
    "    vis_feature,\n",
    "    X=fixed(DATA_DST_FIX['X']),\n",
    "    y=fixed(DATA_DST_FIX['y']),\n",
    "    pids=fixed(DATA_DST_FIX['group']),\n",
    "    feature=widgets.Select(options=sorted(DATA_DST_FIX['X'].columns), rows=15)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve and summarize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "LABEL = load('./proc/label.pkl')\n",
    "\n",
    "for l in LABEL.columns:\n",
    "    dataset = load(f'./proc/dataset.{l}.pkl')\n",
    "    X = dataset['X']\n",
    "    space = X.shape\n",
    "    cat_masks = X.dtypes == 'object'\n",
    "    num_masks = X.dtypes != 'object'\n",
    "    X_cat = X.loc[:, cat_masks]\n",
    "    cardinality = []\n",
    "    \n",
    "    print(f'{\"-\" * 5}{l}{\"-\" * 5}')\n",
    "    print(f'Feature space: {X.shape}')\n",
    "    print(f'- Numeric: {np.sum(num_masks)}')\n",
    "    print(f'- Categorical: {np.sum(cat_masks)}')\n",
    "    \n",
    "    for k, v in X_cat.iteritems():\n",
    "        c = v.unique()\n",
    "        print(f'-- {k}: {c}')\n",
    "        cardinality.append(len(c))\n",
    "    print(f'- Avg. cardinality: {np.mean(cardinality)}')\n",
    "    \n",
    "    k, c = np.unique(LABEL[l], return_counts=True)\n",
    "    print(f'Class values: {k} / Dist. : {c}\\r\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Iterable, Optional\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "class ModifiedXGBClassifier(BaseEstimator):\n",
    "    def __init__(\n",
    "        self,\n",
    "        eval_set=False,\n",
    "        eval_metric='logloss',\n",
    "        early_stopping_rounds=10,\n",
    "        random_state=None,\n",
    "        **kwargs\n",
    "        ):\n",
    "        self.random_state = random_state\n",
    "        self.eval_set = eval_set\n",
    "        self.eval_metric = eval_metric\n",
    "        self.early_stopping_rounds = early_stopping_rounds\n",
    "        self.model = XGBClassifier(\n",
    "            random_state=self.random_state,\n",
    "            eval_metric=self.eval_metric,\n",
    "            early_stopping_rounds=self.early_stopping_rounds,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def classes_(self):\n",
    "        return self.model.classes_\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: np.ndarray):\n",
    "        if self.eval_set:\n",
    "            I_train, I_eval = next(StratifiedShuffleSplit(random_state=self.random_state).split(X, y))\n",
    "            X_train, y_train = X.iloc[I_train, :], y[I_train]\n",
    "            X_eval, y_eval = X.iloc[I_eval, :], y[I_eval]\n",
    "            self.model = self.model.fit(\n",
    "                X=X_train, y=y_train, eval_set=[(X_eval, y_eval)],\n",
    "                verbose=False\n",
    "            )\n",
    "        else:\n",
    "            self.model = self.model.fit(X=X, y=y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: pd.DataFrame):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def predict_proba(self, X: pd.DataFrame):\n",
    "        return self.model.predict_proba(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import traceback as tb\n",
    "from functools import partial\n",
    "from typing import Tuple, Dict, Union, Callable, Generator, List\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC\n",
    "from pyparsing import col\n",
    "import ray\n",
    "from ray import tune as ray_tune\n",
    "from ray.tune.suggest.optuna import OptunaSearch\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import StratifiedKFold, LeaveOneGroupOut, StratifiedShuffleSplit, RepeatedStratifiedKFold, \\\n",
    "    StratifiedGroupKFold\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.base import BaseEstimator\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "import time\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class FoldResult:\n",
    "    name: str\n",
    "    estimator: BaseEstimator\n",
    "    X_train: pd.DataFrame\n",
    "    y_train: np.ndarray\n",
    "    X_test: pd.DataFrame\n",
    "    y_test: np.ndarray\n",
    "    tuned_params: Dict[str, any] = None\n",
    "    categories: Dict[str, Dict[int, str]] = None\n",
    "\n",
    "def _split(\n",
    "        alg: str,\n",
    "        X: Union[pd.DataFrame, np.ndarray] = None,\n",
    "        y: np.ndarray = None,\n",
    "        groups: np.ndarray = None,\n",
    "        random_state: int = None,\n",
    "        n_splits: int = None,\n",
    "        n_repeats: int = None,\n",
    "        test_ratio: float = None,\n",
    "        t_size_s: int = None\n",
    ") -> Generator[Tuple[np.ndarray, np.ndarray], None, None]:\n",
    "    if alg == 'holdout':\n",
    "        splitter = StratifiedShuffleSplit(\n",
    "            n_splits=n_splits,\n",
    "            test_size=test_ratio,\n",
    "            random_state=random_state\n",
    "        )\n",
    "    elif alg == 'kfold':\n",
    "        if n_repeats and n_repeats > 1:\n",
    "            splitter = RepeatedStratifiedKFold(\n",
    "                n_splits=n_splits,\n",
    "                n_repeats=n_repeats,\n",
    "                random_state=random_state,\n",
    "            )\n",
    "        else:\n",
    "            splitter = StratifiedKFold(\n",
    "                n_splits=n_splits,\n",
    "                random_state=random_state,\n",
    "                shuffle=False if random_state is None else True,\n",
    "            )\n",
    "    elif alg == 'logo':\n",
    "        splitter = LeaveOneGroupOut()\n",
    "    elif alg == 'groupk':\n",
    "        splitter = StratifiedGroupKFold(\n",
    "            n_splits=n_splits,\n",
    "            random_state=random_state,\n",
    "            shuffle=False if random_state is None else True\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        raise ValueError('\"alg\" should be one of \"holdout\", \"kfold\", \"logo\", or \"groupk\".')\n",
    "\n",
    "    split = splitter.split(X, y, groups)\n",
    "\n",
    "    for I_train, I_test in split:\n",
    "        yield I_train, I_test\n",
    "\n",
    "\n",
    "def _trainable(\n",
    "    config,\n",
    "    checkpoint_dir=None,\n",
    "    X: pd.DataFrame = None,\n",
    "    y: np.ndarray = None,\n",
    "    categories: List[str] = None,\n",
    "    split: Callable[[pd.DataFrame, np.ndarray], Generator[Tuple[np.ndarray, np.ndarray], None, None]] = None,\n",
    "    fit: Callable[[pd.DataFrame, np.ndarray, Dict[str, any], List[str]], BaseEstimator] = None,\n",
    "    ignore_warning: bool = False,\n",
    "):\n",
    "    losses = []\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        if ignore_warning:\n",
    "            warnings.simplefilter('ignore')\n",
    "\n",
    "        for I_train, I_eval in split(X, y):\n",
    "            X_train, y_train, X_eval, y_eval = X.iloc[I_train, :], y[I_train], X.iloc[I_eval, :], y[I_eval]\n",
    "            estimator = fit(X_train, y_train, config, categories)\n",
    "\n",
    "            if estimator is None:\n",
    "                break\n",
    "\n",
    "            y_pred = estimator.predict_proba(X_eval)\n",
    "            losses.append(log_loss(y_true=y_eval, y_pred=y_pred))\n",
    "\n",
    "    losses = np.nanmean(losses)\n",
    "\n",
    "    if np.isnan(losses):\n",
    "        raise ValueError('Failed to train model.')\n",
    "\n",
    "    ray_tune.report(objective=losses)\n",
    "\n",
    "\n",
    "def _train(\n",
    "    name: str,\n",
    "    X_train_C: np.ndarray,\n",
    "    X_train_N: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    X_test_C: np.ndarray,\n",
    "    X_test_N: np.ndarray,\n",
    "    y_test: np.ndarray,\n",
    "    C_cat: np.ndarray,\n",
    "    C_num: np.ndarray,\n",
    "    fit: Callable[[pd.DataFrame, np.ndarray, Dict[str, any], List[str]], BaseEstimator],\n",
    "    encoder: Union[OneHotEncoder, OrdinalEncoder] = None,\n",
    "    normalize: bool = False,\n",
    "    select: bool = False,\n",
    "    select_threshold: float = 1e-5,\n",
    "    select_params: Dict[str, any] = None,\n",
    "    oversample: bool = False,\n",
    "    tune: bool = False,\n",
    "    tune_search_space: Callable[[optuna.Trial], None] = None,\n",
    "    tune_split: str = None,\n",
    "    tune_split_params=None,\n",
    "    tune_trials: int = -1,\n",
    "    tune_time_s: int = 60 * 5,\n",
    "    tune_path: str = './',\n",
    "    random_state: int = None,\n",
    "    ignore_warning: bool = False,\n",
    "    verbosity: int = 0\n",
    ") -> FoldResult:\n",
    "    def _log(_v, _m):\n",
    "        if _v >= verbosity:\n",
    "            log(_m)\n",
    "    \n",
    "    with warnings.catch_warnings():\n",
    "        if ignore_warning:\n",
    "            warnings.simplefilter('ignore')\n",
    "        _log(1, f'Training is started in the fold, {name}')\n",
    "        \n",
    "        ELAPSED_TIME = time.time()\n",
    "        X_train_C, X_train_N = X_train_C.astype(object), X_train_N.astype(np.float32)\n",
    "        X_test_C, X_test_N = X_test_C.astype(object), X_test_N.astype(np.float32)\n",
    "\n",
    "        if select:\n",
    "            _log(0, f'-- Begin feature selection.')\n",
    "            try:\n",
    "                select_params = select_params or dict()\n",
    "\n",
    "                scaler = StandardScaler().fit(X_train_N)\n",
    "                svc = LinearSVC(\n",
    "                    random_state=random_state, **select_params\n",
    "                ).fit(X=np.nan_to_num(scaler.transform(X_train_N)), y=y_train)\n",
    "                M = np.asarray([abs(r) > select_threshold for r in np.ravel(svc.coef_)])\n",
    "                C_num = C_num[M]\n",
    "                X_train_N, X_test_N = X_train_N[:, M], X_test_N[:, M] \n",
    "                _log(0, f'-- Complete feature selection: # Cat. columns = {len(C_cat)}; # Num. columns = {len(C_num)}')\n",
    "            except:\n",
    "                _log(2, f'-- Fail to select features. Keep training without feature selection. Caused by: \\n{tb.format_exc()}')\n",
    "        \n",
    "        if normalize:\n",
    "            _log(0, f'Begin normalizing numeric features.')\n",
    "\n",
    "            try:\n",
    "                scaler = StandardScaler().fit(X_train_N)\n",
    "                X_train_N = np.nan_to_num(scaler.transform(X_train_N)).astype(np.float32)\n",
    "                X_test_N = np.nan_to_num(scaler.transform(X_test_N)).astype(np.float32)\n",
    "                _log(0, f'Complete to normalizing numeric features.')\n",
    "            except:\n",
    "                _log(2, f'Fail to normalize numeric features. Keep training without it. Caused by: \\n{tb.format_exc()}')\n",
    "\n",
    "        if oversample:\n",
    "            _log(0, f'Begin oversampling.')\n",
    "            try:\n",
    "                ord_enc = OrdinalEncoder(dtype=np.float32).fit(X_train_C)\n",
    "                X_train = np.concatenate((ord_enc.transform(X_train_C), X_train_N), axis=1)\n",
    "                I_cat = np.arange(X_train_C.shape[1])\n",
    "                I_num = np.setdiff1d(np.arange(X_train.shape[1]), I_cat)\n",
    "                if len(C_cat):\n",
    "                    sampler = SMOTENC(categorical_features=I_cat, random_state=random_state)\n",
    "                else:\n",
    "                    sampler = SMOTE(random_state=random_state)\n",
    "                X_train, y_train = sampler.fit_resample(X_train, y_train)\n",
    "                \n",
    "                X_train_C, X_train_N = X_train[:, I_cat], X_train[:, I_num]\n",
    "                X_train_C = ord_enc.inverse_transform(X_train_C)\n",
    "                _log(0, f'Complete oversampling.')\n",
    "            except:\n",
    "                _log(2, f'Fail to oversample. Keep training without oversampling. Caused by: \\n{tb.format_exc()}')\n",
    "        X_train_C, X_test_C = encoder.transform(X_train_C).astype(np.int32), encoder.transform(X_test_C).astype(np.int32)\n",
    "\n",
    "        if isinstance(encoder, OneHotEncoder):\n",
    "            categories = C_cat = list(encoder.get_feature_names_out(C_cat))\n",
    "        else:\n",
    "            categories = {k: {c: i for i, c in enumerate(v)} for k, v in zip(C_cat, encoder.categories_)}\n",
    "            \n",
    "        X_train_C = pd.DataFrame(X_train_C, columns=C_cat, dtype=np.int32)\n",
    "        X_train_N = pd.DataFrame(X_train_N, columns=C_num, dtype=np.float32)\n",
    "        X_test_C = pd.DataFrame(X_test_C, columns=C_cat, dtype=np.int32)\n",
    "        X_test_N = pd.DataFrame(X_test_N, columns=C_num, dtype=np.float32)\n",
    "\n",
    "        X_train = pd.concat((X_train_C, X_train_N), axis=1)\n",
    "        X_test = pd.concat((X_test_C, X_test_N), axis=1)\n",
    "\n",
    "        best_config = dict()\n",
    "\n",
    "        if tune and tune_search_space and tune_split:\n",
    "            _log(0, f'Begin hyperparamter tuning.')\n",
    "\n",
    "            tune_split_params = tune_split_params or dict()\n",
    "            tune_splitter = partial(\n",
    "                _split, alg=tune_split, random_state=random_state,\n",
    "                **tune_split_params\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                objective = ray_tune.with_parameters(\n",
    "                    _trainable,\n",
    "                    X=X_train, \n",
    "                    y=y_train,\n",
    "                    split=tune_splitter,\n",
    "                    fit=fit,\n",
    "                    categories=list(C_cat),\n",
    "                    ignore_warning=ignore_warning,\n",
    "                )\n",
    "                search = OptunaSearch(\n",
    "                    search_space=tune_search_space,\n",
    "                    metric='objective',\n",
    "                    mode='min',\n",
    "                    sampler=TPESampler(seed=random_state)\n",
    "                )\n",
    "\n",
    "                analysis = ray_tune.run(\n",
    "                    objective,\n",
    "                    search_alg=search,\n",
    "                    metric='objective',\n",
    "                    mode='min',\n",
    "                    local_dir=tune_path,\n",
    "                    name=name,\n",
    "                    verbose=1,\n",
    "                    raise_on_failed_trial=False,\n",
    "                    stop=dict(objective=1e-5),\n",
    "                    time_budget_s=tune_time_s,\n",
    "                    num_samples=tune_trials\n",
    "                )\n",
    "                best_config = analysis.best_config or dict()\n",
    "                _log(0, f'Complete hyperparamter tuning.')\n",
    "\n",
    "            except:\n",
    "                _log(2, f'Fail to tune hyperparameters. Keep training with initial parameters. Caused by: \\n{tb.format_exc()}')\n",
    "\n",
    "        try:\n",
    "            _log(0, f'Begin model fitting.')\n",
    "            estimator = fit(X_train, y_train, best_config, list(C_cat))\n",
    "            result = FoldResult(\n",
    "                name=name,\n",
    "                estimator=estimator,\n",
    "                X_train=X_train,\n",
    "                y_train=y_train,\n",
    "                X_test=X_test,\n",
    "                y_test=y_test,\n",
    "                tuned_params=best_config,\n",
    "                categories=categories\n",
    "            )\n",
    "            return result\n",
    "        except:\n",
    "            _log(2, f'Fail to fit a model. Caused by: \\n{tb.format_exc()}')\n",
    "            return None\n",
    "        finally:\n",
    "            _log(1, f'Complete training in the fold, {name} ({time.time() - ELAPSED_TIME:.2f} s).')\n",
    "\n",
    "\n",
    "def cross_val(\n",
    "    X: pd.DataFrame,\n",
    "    y: np.ndarray,\n",
    "    groups: np.ndarray,\n",
    "    path: str,\n",
    "    name: str,\n",
    "    fit: Callable[[pd.DataFrame, np.ndarray, Dict[str, any], List[str]], BaseEstimator],\n",
    "    categories: List[str] = None,\n",
    "    normalize: bool = False,\n",
    "    split: str = None,\n",
    "    split_params: Dict[str, any] = None,\n",
    "    select: bool = False,\n",
    "    select_threshold: float = 1e-5,\n",
    "    select_params: Dict[str, any] = None,\n",
    "    oversample: bool = False,\n",
    "    tune: bool = False,\n",
    "    tune_search_space: Callable[[optuna.Trial], None] = None,\n",
    "    tune_split: str = None,\n",
    "    tune_split_params=None,\n",
    "    tune_trials: int = -1,\n",
    "    tune_time_s: int = 60 * 5,\n",
    "    tune_path: str = './',\n",
    "    random_state: int = None,\n",
    "    ignore_warning: bool = False,\n",
    "    onehot: bool = False,\n",
    "    verbosity: int = 0\n",
    "):\n",
    "    if not os.path.exists(path):\n",
    "        raise ValueError('\"path\" does not exist.')\n",
    "    \n",
    "    if not split:\n",
    "        raise ValueError('\"split\" should be specified.')\n",
    "    \n",
    "    if not ray.is_initialized():\n",
    "        raise EnvironmentError('\"ray\" should be initialized.')\n",
    "    \n",
    "    jobs = []\n",
    "    func = ray.remote(_train).remote\n",
    "\n",
    "    categories = list() if categories is None else categories\n",
    "    C_cat = np.asarray(sorted(categories))\n",
    "    C_num = np.asarray(sorted(X.columns[~X.columns.isin(C_cat)]))\n",
    "\n",
    "    # Encoding categorical features\n",
    "    if onehot:\n",
    "        encoder = OneHotEncoder(dtype=np.int32, sparse=False, drop=None).fit(X[C_cat].values)\n",
    "    else:\n",
    "        encoder = OrdinalEncoder(dtype=np.int32).fit(X[C_cat].values)\n",
    "\n",
    "    split_params = split_params or dict()\n",
    "    splitter = _split(alg=split, X=X, y=y, groups=groups, random_state=random_state, **split_params)\n",
    "\n",
    "    for idx_fold, (I_train, I_test) in enumerate(splitter):\n",
    "        if split == 'logo':\n",
    "            FOLD_NAME = str(np.unique(groups[I_test]).item(0))\n",
    "        else:\n",
    "            FOLD_NAME = str(idx_fold + 1)\n",
    "\n",
    "        X_train, y_train = X.iloc[I_train, :], y[I_train]\n",
    "        X_train_C, X_train_N = X_train[C_cat], X_train[C_num]\n",
    "\n",
    "        X_test, y_test = X.iloc[I_test, :], y[I_test]\n",
    "        X_test_C, X_test_N = X_test[C_cat], X_test[C_num]\n",
    "\n",
    "        job = func(\n",
    "            name=f'{name}.{FOLD_NAME}',\n",
    "            X_train_C=X_train_C.values,\n",
    "            X_train_N=X_train_N.values,\n",
    "            y_train=y_train,\n",
    "            X_test_C=X_test_C.values,\n",
    "            X_test_N=X_test_N.values,\n",
    "            y_test=y_test,\n",
    "            C_cat=C_cat,\n",
    "            C_num=C_num,\n",
    "            fit=fit,\n",
    "            encoder=encoder,\n",
    "            normalize=normalize,\n",
    "            select=select,\n",
    "            select_threshold=select_threshold,\n",
    "            select_params=select_params,\n",
    "            oversample=oversample,\n",
    "            tune=tune,\n",
    "            tune_search_space=tune_search_space,\n",
    "            tune_split=tune_split,\n",
    "            tune_split_params=tune_split_params,\n",
    "            tune_trials=tune_trials,\n",
    "            tune_time_s=tune_time_s,\n",
    "            tune_path=tune_path,\n",
    "            random_state=random_state,\n",
    "            ignore_warning=ignore_warning,\n",
    "            verbosity=verbosity\n",
    "        )\n",
    "        jobs.append(job)\n",
    "\n",
    "    jobs = ray.get(jobs)\n",
    "\n",
    "    dump(jobs, os.path.join(path, f'{name}.pkl'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measurements for model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For the origianl class distribution\n",
    "    * Majority Voting from DummyClassifier\n",
    "* For nominal class measures:\n",
    "    * Accuracy, Balanced Accuracy, F1 (Y), F1 (N)\n",
    "* For ranking measures:\n",
    "    * AU-ROC (ref. = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, \\\n",
    "    confusion_matrix, precision_recall_fscore_support, \\\n",
    "    roc_auc_score, matthews_corrcoef, average_precision_score, \\\n",
    "    log_loss, brier_score_loss\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "import scipy.stats.mstats as ms\n",
    "from typing import Dict\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    y_true: np.ndarray,\n",
    "    y_pred: np.ndarray,\n",
    "    y_prob: np.ndarray,\n",
    "    classes: np.ndarray\n",
    ") -> Dict[str, any]:\n",
    "    R = {}\n",
    "    n_classes = len(classes)\n",
    "    is_multiclass = n_classes > 2\n",
    "\n",
    "    R['inst'] = len(y_true)\n",
    "    \n",
    "    for c in classes:\n",
    "        R[f'inst_{c}'] = np.sum(y_true == c)\n",
    "\n",
    "    C = confusion_matrix(y_true=y_true, y_pred=y_pred, labels=classes)\n",
    "    for (i1, c1), (i2, c2) in product(enumerate(classes), enumerate(classes)):\n",
    "        R[f'true_{c1}_pred_{c2}'] = C[i1, i2]\n",
    "\n",
    "    # Threshold Measure\n",
    "    R['acc'] = accuracy_score(y_true=y_true, y_pred=y_pred)\n",
    "    R['bac'] = balanced_accuracy_score(y_true=y_true, y_pred=y_pred)\n",
    "    R['gmean'] = ms.gmean(np.diag(C) / np.sum(C, axis=1))\n",
    "    R['mcc'] = matthews_corrcoef(y_true=y_true, y_pred=y_pred)\n",
    "    if is_multiclass:\n",
    "        for avg in ('macro', 'micro'):\n",
    "            pre, rec, f1, _ = precision_recall_fscore_support(\n",
    "                y_true=y_true,\n",
    "                y_pred=y_pred,\n",
    "                labels=classes,\n",
    "                average=avg, \n",
    "                zero_division=0\n",
    "            )\n",
    "            R[f'pre_{avg}'] = pre\n",
    "            R[f'rec_{avg}'] = rec\n",
    "            R[f'f1_{avg}'] = f1\n",
    "    else:\n",
    "        for c in classes:\n",
    "            pre, rec, f1, _ = precision_recall_fscore_support(\n",
    "                y_true=y_true, y_pred=y_pred, pos_label=c, average='binary', zero_division=0\n",
    "            )\n",
    "            R[f'pre_{c}'] = pre\n",
    "            R[f'rec_{c}'] = rec\n",
    "            R[f'f1_{c}'] = f1\n",
    "\n",
    "    # Ranking Measure\n",
    "    if is_multiclass:\n",
    "        for avg, mc in product(('macro', 'micro'), ('ovr', 'ovo')):\n",
    "            R[f'roauc_{avg}_{mc}'] = roc_auc_score(\n",
    "                y_true=y_true, y_score=y_prob,\n",
    "                average=avg, multi_class=mc, labels=classes\n",
    "            )\n",
    "    else:\n",
    "        R[f'roauc'] = roc_auc_score(\n",
    "            y_true=y_true, y_score=y_prob[:, 1], average=None\n",
    "        )\n",
    "        for i, c in enumerate(classes):\n",
    "            R[f'prauc_{c}'] = average_precision_score(\n",
    "                y_true=y_true, y_score=y_prob[:, i], pos_label=c, average=None\n",
    "            )\n",
    "            R[f'prauc_ref_{c}'] = np.sum(y_true == c) / len(y_true)\n",
    "\n",
    "    # Probability Measure\n",
    "    R['log_loss'] = log_loss(y_true=y_true, y_pred=y_prob, labels=classes, normalize=True)\n",
    "\n",
    "    if not is_multiclass:\n",
    "        R[f'brier_loss'] = brier_score_loss(\n",
    "            y_true=y_true, y_prob=y_prob[:, 1], pos_label=classes[1]\n",
    "        )\n",
    "\n",
    "    return R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building and evaluating the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from functools import partial\n",
    "from itertools import combinations, chain\n",
    "import altair as alt\n",
    "\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "FIT_DUMMY = lambda X, y, params, _: DummyClassifier(strategy='prior').fit(X, y)\n",
    "\n",
    "FIT_RF = lambda X, y, params, _: RandomForestClassifier(random_state=RANDOM_STATE, **params).fit(X, y)\n",
    "\n",
    "FIT_XGB = lambda X, y, params, _: ModifiedXGBClassifier(\n",
    "    random_state=RANDOM_STATE, \n",
    "    eval_set=True, eval_metric='logloss', early_stopping_rounds=10,\n",
    "    objective='binary:logistic', verbosity=0,\n",
    "    **params\n",
    ").fit(X, y)\n",
    "\n",
    "FITS = [\n",
    "    ('DUMMY', FIT_DUMMY, True, False),\n",
    "    ('RF', FIT_RF, True, True),\n",
    "    ('XGB', FIT_XGB, True, True)\n",
    "]\n",
    "\n",
    "CROSS_VAL_BASE = partial(\n",
    "    cross_val,\n",
    "    normalize=True,\n",
    "    select=True, \n",
    "    select_threshold=1e-3,\n",
    "    select_params=dict(tol=1e-3, max_iter=5000, dual=False, penalty='l1', loss='squared_hinge', C=1e-2),\n",
    "    tune=False,\n",
    "    oversample=True,\n",
    "    random_state=RANDOM_STATE,\n",
    "    ignore_warning=True,\n",
    "    verbosity=1\n",
    ")\n",
    "\n",
    "LABELS = ['val_fix', 'aro_fix', 'sts_fix', 'dst_fix']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10-Fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "\n",
    "with on_ray(ignore_reinit_error=True, num_cpus=7):\n",
    "    for l, (model, fit, onehot, oversample) in product(LABELS, FITS):\n",
    "        DATA = load(f'./proc/dataset.{l}.pkl')\n",
    "        X, y, groups = DATA['X'], DATA['y'], DATA['group']\n",
    "        C = X.columns\n",
    "        M = np.ones(len(C), dtype=np.bool8)\n",
    "\n",
    "        CROSS_VAL_BASE(\n",
    "            name= f'{l}.{model}',\n",
    "            path='./eval/kfold', \n",
    "            X=X, \n",
    "            y=y, \n",
    "            groups=groups,\n",
    "            fit=fit, \n",
    "            split='kfold',\n",
    "            split_params=dict(\n",
    "                n_splits=10\n",
    "            ),\n",
    "            categories=list(C[X.dtypes == object]),\n",
    "            onehot=onehot,\n",
    "            oversample=oversample\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group 10-Fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "with on_ray(ignore_reinit_error=True, num_cpus=7):\n",
    "    for l, (model, fit, onehot, oversample) in product(LABELS, FITS):\n",
    "        DATA = load(f'./proc/dataset.{l}.pkl')\n",
    "        X, y, groups = DATA['X'], DATA['y'], DATA['group']\n",
    "        C = X.columns\n",
    "        M = np.ones(len(C), dtype=np.bool8)\n",
    "\n",
    "        CROSS_VAL_BASE(\n",
    "            name= f'{l}.{model}',\n",
    "            path='./eval/groupk', \n",
    "            X=X, \n",
    "            y=y, \n",
    "            groups=groups,\n",
    "            fit=fit, \n",
    "            split='groupk',\n",
    "            split_params=dict(\n",
    "                n_splits=10\n",
    "            ),\n",
    "            categories=list(C[X.dtypes == object]),\n",
    "            onehot=onehot,\n",
    "            oversample=oversample\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "\n",
    "\n",
    "def vis_measure(X: pd.DataFrame, measure: str):\n",
    "    X = X.loc[lambda x: (x['metric'] == measure), :]\n",
    "    \n",
    "    return alt.Chart(X).mark_boxplot().encode(\n",
    "        x=alt.X('model:N', title='Model'),\n",
    "        y=alt.Y('value:Q', title='Value'),\n",
    "        column=alt.Column('label:N', title='Label')\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10-Fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import ray\n",
    "from typing import List\n",
    "\n",
    "\n",
    "EVAL_KFOLD = []\n",
    "\n",
    "for p in os.listdir('./eval/kfold/'):\n",
    "    result = load(os.path.join('./eval/kfold/', p))\n",
    "    parts = p.split('.')\n",
    "    label, model = parts[0], parts[1]\n",
    "\n",
    "    for r in result:\n",
    "        name = r.name.split('.')[-1]\n",
    "        X, y = r.X_test, r.y_test\n",
    "        y_pred = r.estimator.predict(X)\n",
    "        y_prob = r.estimator.predict_proba(X)\n",
    "        classes = r.estimator.classes_\n",
    "        n_features = len(r.X_test.columns)\n",
    "        measures = evaluate(y, y_pred, y_prob, classes)\n",
    "\n",
    "        EVAL_KFOLD.append({\n",
    "            'label': label,\n",
    "            'model': model,\n",
    "            'fold': name,\n",
    "            'n_features': n_features,\n",
    "            **measures\n",
    "        })\n",
    "\n",
    "EVAL_KFOLD = pd.DataFrame(EVAL_KFOLD).melt(\n",
    "    id_vars=['label', 'model', 'fold'],\n",
    "    var_name='metric',\n",
    "    value_name='value'\n",
    ")\n",
    "\n",
    "EVAL_KFOLD.to_csv('./eval/kfold-metric.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import fixed, interact\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "EVAL_KFOLD = pd.read_csv('./eval/kfold-metric.csv')\n",
    "interact(\n",
    "    vis_measure,\n",
    "    X=fixed(EVAL_KFOLD),\n",
    "    measure=EVAL_KFOLD['metric'].unique()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group 10-Fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import ray\n",
    "from typing import List\n",
    "\n",
    "\n",
    "EVAL_GROUPK = []\n",
    "\n",
    "for p in os.listdir('./eval/groupk/'):\n",
    "    result = load(os.path.join('./eval/groupk/', p))\n",
    "    if p.endswith('csv'):\n",
    "        continue\n",
    "    parts = p.split('.')\n",
    "    label, model = parts[0], parts[1]\n",
    "\n",
    "    for r in result:\n",
    "        name = r.name.split('.')[-1]\n",
    "        X, y = r.X_test, r.y_test\n",
    "        y_pred = r.estimator.predict(X)\n",
    "        y_prob = r.estimator.predict_proba(X)\n",
    "        classes = r.estimator.classes_\n",
    "        n_features = len(r.X_test.columns)\n",
    "        measures = evaluate(y, y_pred, y_prob, classes)\n",
    "\n",
    "        EVAL_GROUPK.append({\n",
    "            'label': label,\n",
    "            'model': model,\n",
    "            'fold': name,\n",
    "            'n_features': n_features,\n",
    "            **measures\n",
    "        })\n",
    "\n",
    "EVAL_GROUPK = pd.DataFrame(EVAL_GROUPK).melt(\n",
    "    id_vars=['label', 'model', 'fold'],\n",
    "    var_name='metric',\n",
    "    value_name='value'\n",
    ")\n",
    "EVAL_GROUPK.to_csv('./eval/groupk-metric.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import fixed, interact\n",
    "\n",
    "\n",
    "EVAL_GROUPK = pd.read_csv('./eval/groupk-metric.csv')\n",
    "interact(\n",
    "    vis_measure,\n",
    "    X=fixed(EVAL_GROUPK),\n",
    "    measure=EVAL_GROUPK['metric'].unique()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {\"val\": \"Valence\",\n",
    "             \"aro\": \"Arousal\",\n",
    "             \"sts\": \"Stress\",\n",
    "             \"dst\": \"Disturbance\"\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10-Fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ray\n",
    "from typing import List\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "IMP_KFOLD = []\n",
    "\n",
    "for p in os.listdir('./eval/kfold/'):\n",
    "    result = load(os.path.join('./eval/kfold/', p))\n",
    "    parts = p.split('.')\n",
    "    label, model = parts[0], parts[1]\n",
    "    \n",
    "    for r in result:\n",
    "        estimator = r.estimator\n",
    "        if isinstance(estimator, DummyClassifier):\n",
    "            continue\n",
    "            \n",
    "        name = r.name.split('.')[-1]\n",
    "        cols = r.X_test.columns\n",
    "        \n",
    "        if isinstance(estimator, RandomForestClassifier):\n",
    "            imp = estimator.feature_importances_\n",
    "        elif isinstance(estimator, CalibratedClassifierCV):\n",
    "            imp = []\n",
    "            \n",
    "            for c in estimator.calibrated_classifiers_:\n",
    "                base_estimator = c.base_estimator\n",
    "                if hasattr(base_estimator, 'feature_importances_'):\n",
    "                    imp.append(base_estimator.feature_importances_)\n",
    "                else:\n",
    "                    imp.append(base_estimator.model.feature_importances_)\n",
    "            imp = np.mean(np.vstack(imp), axis=0)\n",
    "        else:\n",
    "            imp = estimator.model.feature_importances_\n",
    "        \n",
    "        imp = {k: v for k, v in zip(cols, imp)}\n",
    "\n",
    "        IMP_KFOLD.append({\n",
    "            'label': label,\n",
    "            'model': model,\n",
    "            'fold': name,\n",
    "            **imp\n",
    "        })\n",
    "\n",
    "IMP_KFOLD = pd.DataFrame(IMP_KFOLD).melt(\n",
    "    id_vars=['label', 'model', 'fold'],\n",
    "    var_name='feature',\n",
    "    value_name='importance'\n",
    ").fillna(0)\n",
    "\n",
    "IMP_KFOLD.to_csv('./eval/kfold-importance.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import altair as alt\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "IMP_KFOLD = pd.read_csv('./eval/kfold-importance.csv')\n",
    "\n",
    "TOP_IMP_KFOLD = IMP_KFOLD.groupby(\n",
    "    ['label', 'model', 'feature'], as_index=False\n",
    ").mean().sort_values(\n",
    "    ['label', 'model', 'importance'], ascending=False\n",
    ").groupby(\n",
    "    ['label', 'model'], as_index=False\n",
    ").head(10)\n",
    "\n",
    "charts = []\n",
    "\n",
    "for label in TOP_IMP_KFOLD['label'].unique():\n",
    "    rows = []\n",
    "    \n",
    "    for model in TOP_IMP_KFOLD['model'].unique():\n",
    "        X = TOP_IMP_KFOLD.loc[\n",
    "            lambda x: (x['label'] == label) & (x['model'] == model), :\n",
    "        ]\n",
    "        c = alt.Chart(X).mark_bar().encode(\n",
    "            x=alt.X('feature:N', title='', sort='-y'),\n",
    "            y=alt.Y('importance:Q', title='Importance')\n",
    "        ).properties(\n",
    "            title=f'{label_map[label.split(\"_\")[0]]}-{model}',\n",
    "            width=200,\n",
    "            height=100\n",
    "        )\n",
    "        rows.append(c)\n",
    "    rows = alt.hconcat(*rows)\n",
    "    charts.append(rows)\n",
    "\n",
    "alt.vconcat(*charts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group 10-Fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ray\n",
    "from typing import List\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "IMP_GROUPK = []\n",
    "\n",
    "for p in os.listdir('./eval/groupk/'):\n",
    "    result = load(os.path.join('./eval/groupk/', p))\n",
    "    parts = p.split('.')\n",
    "    label, model = parts[0], parts[1]\n",
    "    \n",
    "    for r in result:\n",
    "        estimator = r.estimator\n",
    "        if isinstance(estimator, DummyClassifier):\n",
    "            continue\n",
    "            \n",
    "        name = r.name.split('.')[-1]\n",
    "        cols = r.X_test.columns\n",
    "        \n",
    "        if isinstance(estimator, RandomForestClassifier):\n",
    "            imp = estimator.feature_importances_\n",
    "        elif isinstance(estimator, CalibratedClassifierCV):\n",
    "            imp = []\n",
    "            \n",
    "            for c in estimator.calibrated_classifiers_:\n",
    "                base_estimator = c.base_estimator\n",
    "                if hasattr(base_estimator, 'feature_importances_'):\n",
    "                    imp.append(base_estimator.feature_importances_)\n",
    "                else:\n",
    "                    imp.append(base_estimator.model.feature_importances_)\n",
    "            imp = np.mean(np.vstack(imp), axis=0)\n",
    "        else:\n",
    "            imp = estimator.model.feature_importances_\n",
    "        \n",
    "        imp = {k: v for k, v in zip(cols, imp)}\n",
    "\n",
    "        IMP_GROUPK.append({\n",
    "            'label': label,\n",
    "            'model': model,\n",
    "            'fold': name,\n",
    "            **imp\n",
    "        })\n",
    "\n",
    "IMP_GROUPK = pd.DataFrame(IMP_GROUPK).melt(\n",
    "    id_vars=['label', 'model', 'fold'],\n",
    "    var_name='feature',\n",
    "    value_name='importance'\n",
    ").fillna(0)\n",
    "\n",
    "IMP_GROUPK.to_csv('./eval/groupk-importance.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import altair as alt\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "IMP_GROUPK = pd.read_csv('./eval/groupk-importance.csv')\n",
    "\n",
    "TOP_IMP_GROUPK = IMP_GROUPK.groupby(\n",
    "    ['label', 'model', 'feature'], as_index=False\n",
    ").mean().sort_values(\n",
    "    ['label', 'model', 'importance'], ascending=False\n",
    ").groupby(\n",
    "    ['label', 'model'], as_index=False\n",
    ").head(10)\n",
    "\n",
    "charts = []\n",
    "\n",
    "for label in TOP_IMP_GROUPK['label'].unique():\n",
    "    rows = []\n",
    "    \n",
    "    for model in TOP_IMP_GROUPK['model'].unique():\n",
    "        X = TOP_IMP_GROUPK.loc[\n",
    "            lambda x: (x['label'] == label) & (x['model'] == model), :\n",
    "        ]\n",
    "        c = alt.Chart(X).mark_bar().encode(\n",
    "            x=alt.X('feature:N', title='', sort='-y'),\n",
    "            y=alt.Y('importance:Q', title='Importance')\n",
    "        ).properties(\n",
    "            title=f'{label_map[label.split(\"_\")[0]]}-{model}',\n",
    "            width=200,\n",
    "            height=100\n",
    "        )\n",
    "        rows.append(c)\n",
    "    rows = alt.hconcat(*rows)\n",
    "    charts.append(rows)\n",
    "\n",
    "alt.vconcat(*charts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
